\documentclass{article}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[main=english,russian]{babel}	
\usepackage{arxiv}

\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newtheorem{theorem}{Theorem}

\title{On the problem of repeated supervised learning}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
    Andrey S. Veprikov\\
	Department of Intelligent Systems\\
	MIPT\\
	Dolgoprudny, Russia \\
	\texttt{veprikov.as@phystech.edu} 
    \And
    Anton S. Kritanlov\\
    HSE University\\
    Moscow, Russia\\
    \texttt{akhritankov@hse.ru}\\
    \And
    Alexander P. Afanasyev \\
    IITP\\
    Moscow, Russia}

\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{On the problem of repeated supervised learning}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf

\begin{document}
\maketitle

\begin{abstract}

    In this research paper, we delve into the intricacies of continuous learning artificial intelligence systems as they interact with and influence their environment. We develop a mathematical model to examine the process of repeated and multiple learning, prediction, and dataset updating. Our investigation of this process is based on the principles of functional analysis and probability theory, which is a novel approach to this problem. We aim to conduct several synthetic experiments based on our findings, hoping to contribute to a better understanding of the behavior of continuous learning AI systems.

\end{abstract}


% keywords can be removed
\keywords{Machine learning \and Continuous machine learning \and Repeated learning}

\section{Introduction} \label{Introduction}

    In this paper we consider the problem of repeated supervised learning \begin{otherlanguage}{russian}(многократное машинное обучение)\end{otherlanguage}, in which the training sample is not fixed, but is updated depending on the predictions of the trained model on the test sample \cite{ma2020machine, khritankov2021existence, jiang2019degenerate}. In many applications, the use of multiple learning techniques can actually lead to suboptimal results. Our main goal was to present a mathematical theory that explains why combining multiple learning algorithms can sometimes hinder their effectiveness. Repeated supervised learning appears in many machine learning applications, for example in recommendation systems \cite{khritankov2021existence, sinha2016deconvolving}, healthcare \cite{adam2020hidden} and predictive policing \cite{ensign2018runaway}. 

    The object of our research will be the set $\mathbf{R}$ of distribution density functions

    \begin{equation}
        \label{R}
        \mathbf{R} := \left\{f : \mathbb{R}^n \rightarrow \mathbb{R}_+ ~\text{and}~ \int\limits_{\mathbb{R}^n}f(x)dx = 1\right\},
    \end{equation}

    and a mapping $\text{D}$, a feedback loop mapping, that includes training a model, forming predictions on a test sample and mixing predictions into a training sample, as a result of which the distribution of features changes.

    A range of problems, that we are considering in this paper are conditions under which $\text{D}$ translates $\mathbf{R}$ into $\mathbf{R}$, conditions when $\text{D}$ is a contraction mapping and its fixed point existence conditions.

    Main contributions ...
    
    Structure of this paper are as follows. In Section \ref{Related_work} we compare our article with the works of other authors and show its novelty to the literature. In Section \ref{Problem_statement} we build a mathematical model for the process of multiple learning, prediction and updating of the sample and outline the main questions, the answers to which we explore in Section \ref{Main_results}. In Section \ref{Main_results} we provide our main contributions for the mapping $\text{D}$. In Section \ref{Experiments}, based on the results from Section \ref{Main_results}, we conduct some synthetic experiments.

\section{Related work} \label{Related_work}    

    The problem we study is somewhat related to the feedback loops \cite{khritankov2021hidden, khritankov2021existence} -- an observable change in the distribution of input data that occurs over time, because of user interaction with the system. According to Conjecture 1 from \cite{khritankov2021hidden} the positive feedback loop in a system $\text{D} : R \rightarrow R$ exists if $\text{D}$ is a contraction mapping, but this conjecture was not proved.

    Also our problem is connected with dynamical systems, which are studied in \cite{katok1995introduction, nemytskii2015qualitative}, but all these works consider continuous time, and in our work it should be discrete.

    Some authors analyzed Markov processes from the point of view of dynamical systems \cite{tarlowski2017global, vershik2005does}. But in Markov chain the future of the process does not depend on the past. Other authors \cite{varvenne2019rate, pap1996fixed} studied stochastic dynamics systems in general.

    An important contribution of this paper is that ...

\section{Problem statement} \label{Problem_statement}

    We consider $\mathbf{R}$ \eqref{R} -- the set of distribution density functions and a mapping $\text{D}$ representing an algorithm for training a model, forming predictions on a test sample and mixing predictions into a training sample, as a result of which the distribution of features changes.

    Let's consider an autonomous (time-independent explicitly) discrete dynamical system. There is a dedicated variable - the step number, which increases, at step $t$ and $t+1$ of which the ratio is fulfilled

    \begin{equation*}
        f_{t+1}(x) = \text{D}(f_t)(x), \quad \text{for}~ \forall x \in \mathbb{R}^n,
    \end{equation*}

    where $\text{D}$ becomes an evolution operator on the space of the specified functions $f$ and the initial function $f_0(x)$ is known. Generally speaking, $\text{D}$ can be an arbitrary mapping, not necessarily smooth or continuous.

    In this paper we consider the following research questions:

    \begin{enumerate}
        \item[\textbf{1.}]  What are the conditions so that the mapping $\text{D}$ becomes a transformation $\mathbf{R}$ into $\mathbf{R}$? 

        \item[\textbf{2.}] Under what conditions the mapping $\text{D}$ is a contraction mapping?

        \item[\textbf{3.}] Under what conditions the mapping $\text{D}$ has a fixed point?
    \end{enumerate}

    Let's analyze the importance of each question. We consider $\text{D}$ as operator of changing distribution of our dataset, so it has to transform $\mathbf{R}$ into $\mathbf{R}$. If $\text{D}$ is a contraction mapping, then for any starting function $f_0$ after a sufficiently large T, the result of our algorithm after $T$ steps would be $D^T(f_0) = f^*$, where $f^*$ is a fixed point, so after $T$ steps metrics of our algorithm cannot be improved. If $\text{D}$ has a fixed point $f$, when if $f_0 = f$ and we cannot improve our metrics as well.


\section{Main results} \label{Main_results}

\section{Experiments} \label{Experiments}

\bibliographystyle{plain}
\bibliography{refs}  

\end{document}